# 9장: 웹 로봇

웹 로봇: 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램

## 9.1 크롤러와 크롤링
웹 크롤러: 웹페이지를 한 개 가져오고, 그 페이지가 가리키는 모든 웹페이지를 가져오는 일들을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇.

인터넷 검색엔진은 크롤러를 사용해서 검색 가능한 데이터베이스로 만든다.


### 9.1.1 어디에서 시작하는가: '루트 집합'

* 루트 집합: 크롤러가 방문을 시작하는 URL들의 초기집합

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려지지 않은 페이지들의 목록으로 구성된다.
루트 집합은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 된다.


### 9.1.2 링크 추출과 상대 링크 정상화

크롤러는 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다.
크롤러들은 간단한 HTML 파싱을 해서 링크를 추출하고 상대 링크를 절대 링크로 변환한다.


### 9.1.3 순환 피하기

로봇이 웹을 크롤링할 때, 루프나 순환에 빠지지 않도록 조심해야 한다. 따라서 이전에 어디를 방문했는지 알아야 한다.


### 9.1.4 루프와 반복

* 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 한다.
* 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다.
* 비록 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지들을 가져오게 된다.


### 9.1.5 빵 부스러기의 흔적

어떤 URL을 방문했었는지 계속 추적하는 작업은 도전적인 일이다. 이를 위해 복잡한 자료 구조를 사용해야 하고 이는 속도와 메모리 사용 면에서 효과적이어야 한다.
URL 목록의 완벽한 검색은 불가능하고, 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 검색 트리나 해시 테이블을 필요로 한다.

* 트리와 해시 테이블
* 느슨한 존재 비트맵
* 체크 포인트
* 파티셔닝


거대한 자료 구조를 구현하기 위한 참고 도서는 위튼(Witten) 등이 쓴 `Managing Gigabytes: Compressing and Indexing Documents and Images`


### 9.1.6 별칭(alias)와 로봇 순환

같은 문서를 가리키는 다른 URL이 있을 경우, 이전에 방문했는 지 여부를 체크하는 게 쉽지 않을 수 있다.


### 9.1.7 URL 정규화하기

1. 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환한다.
3. 태그들을 제거한다.

URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만 이는 모든 URL 별칭을 커버하진 못한다.


### 9.1.8 파일 시스템 링크 순환

악의적으로 순환하는 시스템을 만들어서 로봇을 순환에 빠지게 할 수 있다.
예를 들면 http://www.foo.com/subdir/subdir/subdir/index.html 이런 식으로.


### 9.1.9 동적 가상 웹 공간

웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 동적 콘텐츠를 통한 크롤러 함정을 만들 수도 있다.
콘텐츠의 동적 성질을 이해하지 못하는 로봇이라면 무한히 다음 콘텐츠를 요청할 수도 있다.


### 9.1.10 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없다. 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 '손실'을 유발할 수 있다.

웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법

* URL 정규화
* 너비 우선 크롤링
* 스로틀링
* URL 크기 제한
* URL/사이트 블랙리스트
* 패턴 발견
* 콘텐츠 지문(fingerprint)
* 사람의 모니터링



## 9.2 로봇의 HTTP

### 9.2.1 요청 헤더 식별하기

* User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
* From: 로봇의 사용자 / 관리자의 이메일 주소를 제공한다.
* Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
* Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공한다.


### 9.2.2 가상 호스팅

로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼져있는 현실에서, 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.


### 9.2.3 조건부 요청

수십억 개의 웹 페이지를 다운 받게 될 수도 있는 인터넷 검색 엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 한다. 
시간이나 엔터티 태그를 비교해서 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.


### 9.2.4 응답 다루기

대다수의 로봇은 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이라 응답 다루기라고 부를 만한 일은 거의 하지 않는다. 
하지만 조건부 요청과 같은 특정 몇몇 기능을 사용하거나 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알아야 한다.

* 상태 코드
* 엔터티


### 9.2.5 User-Agent 타겟팅

웹 관리자들은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고, 로봇들로부터의 요청을 예상해야 한다.
사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다. 


## 9.3 부적절하게 동작하는 로봇들

#### 폭주하는 로봇 

로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있다.
모든 로봇 저자들은 폭주 방지를 위한 보호장치를 반드시 신경 써서 설계해야 한다.

#### 오래된 URL

URL의 목록을 방문할 때 그 목록이 오래되었을 수 있다. 웹 사이트가 콘텐츠를 많이 바꾸었다면, 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.

#### 길고 잘못된 URL

크고 의미 없는 URL을 요청할 수 있다. 

#### 호기심이 지나친 로봇

어떤 로봇들은 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수 있다.

#### 동적 게이트웨이 접근

로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다.


## 9.4 로봇 차단하기

로봇이 그들에게 맞지 않는 장소에 들어오지 않도록 하고 웹 마스터에게 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안되었다. 이 표준은 `Robots Exclusion Standard`라고 이름 지어졌지만, `robots.txt`라고 불린다.

어떤 웹 서버는 서버의 문서 루트에 robots.txt라는 선택적인 파일을 제공할 수 있다. 
이 파일엔 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.

### 9.4.1 로봇 차단 표준

* 0.0: 로봇 배재 표준-Disallow 지시자를 지원하는 카틴 코스터의 오리지널 robots.txt 매커니즘
* 1.0: 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안
* 2.0: 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원되지는 않는다.


### 9.4.2 웹 사이트와 robots.txt 파일들

URL을 방문하기 전에 그 웹 사이트에 robots.txt 파일이 존재하면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 

#### robots.txt 가져오기

로봇은 HTTP GET 메서드를 이용해 robots.txt를 가져온다. 존재한다면 서버는 그 파일을 text/plain 본문으로 반환한다.

#### 응답 코드

많은 웹 사이트가 robots.txt를 갖고 있지 않지만, 로봇은 그 사실을 모른다.

* 서버가 성공으로 으답하면 그 응답의 콘텐츠를 파싱해서 차단 규칙을 얻고, 그 규칙에 따라 무언가를 가져와야 한다.
* 존재하지 않는다고 응답하면 로봇은 활성화된 차단 규칙이 존재하지 않는다고 가정하고 제약 없이 사이트에 접근할 수 있다.
* 접근 제한(401 or 403)으로 응답하면 그 사이트로의 접근은 완전히 제한되어 있다고 가정해야 한다.
* 요청 시도가 일시적으로 실패했다면(503) 그 사이트의 리소스를 검색하는 것은 뒤로 미루어야 한다.
* 서버 응답이 리다이렉션을 의미한다면(3XX) 로봇은 리소스가 발견될 때까지 리다이렉트를 따라가야 한다.


### 9.4.3 robots.txt 파일 포맷

```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

대응하는 User-Agent 가 없다면 접근에 아무 제한이 없다. Disallow와 Allow 줄은 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되고 허용되는지 기술한다.


### 9.4.4 그 외에 알아둘 점

* robots.txt 파일은 명세가 발전함에 따라 User-Agent, Disallow, Allow 외의 다른 필드를 포함할 수 있다. 자신이 이해하지 못하는 필드는 무시한다.
* 하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않는다.
* 주석은 파일의 어디에서든 허용된다. # 에서 줄바꿈 문자까지가 주석.
* 버전 0.0은 Allow 줄을 지원하지 않았다.


### 9.4.5 robots.txt의 캐싱과 만료

주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 한다. 
로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다.
오늘날 많은 크롤러 제품들은 HTTP/1.1 클라이언트가 아니기 때문에 웹 마스터들은 크롤러들이 robots.txt 리소스에 적용되는 캐시 지시자를 이해하지 못할 수도 있다는 점을 주의해야 한다.

로봇 명세 초안은 Cache-Control 지시자가 존재하는 경우 7일간 캐싱하도록 하지만 이는 실무에서 너무 길다. 


### 9.4.6 로봇 차단 펄 코드
### 9.4.7 HTML 로봇 제어 META 태그

`<META NAME="ROBOTS" CONTENT=directive-list>`

* NOINDEX
* NOFOLLOW
* INDEX
* FOLLOW
* NOARCHIVE
* ALL
* NONE

#### 검색엔진 META 태그

* DESCRIPTION
* KEYWORDS
* REVISIT-AFTER


## 9.5 로봇 에티켓

로봇 제작자들을 위한 가이드라인(Guidelines for Robot Writers)


## 9.6 검색엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다.
웹 크롤러들은 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다 주어서, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다.


### 9.6.1 넓게 생각하라

대규모 크롤러가 자신의 작업을 완료하려면 많은 장비를 똑똑하게 사용해서 요청을 병렬로 수행할 수 있어야 할 것이라는 점은 명백하다.
규모 때문에 웹 전체를 크롤링하는 것은 쉽지 않은 도전이다.


### 9.6.2 현대적인 검색엔진의 아키텍처

풀 텍스트 색인(full-text indexes)라는 복잡한 로컬 데이터베이스를 생성한다. 이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작한다.


### 9.6.3 풀 텍스트 색인

풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다. 


### 9.6.4 질의 보내기

### 9.6.5 검색 결과를 정렬하고 보여주기

검색엔진은 결과에 순위를 매기기 위해 똑똑한 알고리즘을 사용한다.
관련이 많은 순서대로 결과 문서에 나타낼 수 있도록 문서들 간의 순서를 알 필요가 있다.
이것은 관련도 랭킹(relevancy ranking)이라고 불리며, 검색 결과의 목록에 점수를 매기고 정렬하는 과정이다.

이 과정을 잘 지원하기 위해 많은 검색엔진이 웹을 크롤링하는 과정에서 수집된 통계 데이터를 실제로 사용한다.


### 9.6.6 스푸핑

검색 결과에서 더 높은 순위를 차지하고자 하는 바람은 검색 시스템과의 게임으로 이어졌고, 검색엔진과 자신의 사이트를 눈에 띄게 할 방법을 찾고 있는 사람들 간의 끝나지 않는 줄다리기를 만들어냈다.
검색엔진과 로봇 구현자들은 이러한 속임수를 더 잘 잡아내기 위해 끊임없이 관련도 알고리즘을 수정해야 한다.
