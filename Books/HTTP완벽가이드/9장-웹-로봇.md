# 9장: 웹 로봇

웹 로봇: 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램

## 9.1 크롤러와 크롤링
웹 크롤러: 웹페이지를 한 개 가져오고, 그 페이지가 가리키는 모든 웹페이지를 가져오는 일들을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇.

인터넷 검색엔진은 크롤러를 사용해서 검색 가능한 데이터베이스로 만든다.


### 9.1.1 어디에서 시작하는가: '루트 집합'

* 루트 집합: 크롤러가 방문을 시작하는 URL들의 초기집합

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려지지 않은 페이지들의 목록으로 구성된다.
루트 집합은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 된다.


### 9.1.2 링크 추출과 상대 링크 정상화

크롤러는 검색한 각 페이지 안에 있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다.
크롤러들은 간단한 HTML 파싱을 해서 링크를 추출하고 상대 링크를 절대 링크로 변환한다.


### 9.1.3 순환 피하기

로봇이 웹을 크롤링할 때, 루프나 순환에 빠지지 않도록 조심해야 한다. 따라서 이전에 어디를 방문했는지 알아야 한다.


### 9.1.4 루프와 반복

* 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 한다.
* 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다.
* 비록 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지들을 가져오게 된다.


### 9.1.5 빵 부스러기의 흔적

어떤 URL을 방문했었는지 계속 추적하는 작업은 도전적인 일이다. 이를 위해 복잡한 자료 구조를 사용해야 하고 이는 속도와 메모리 사용 면에서 효과적이어야 한다.
URL 목록의 완벽한 검색은 불가능하고, 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 검색 트리나 해시 테이블을 필요로 한다.

* 트리와 해시 테이블
* 느슨한 존재 비트맵
* 체크 포인트
* 파티셔닝


거대한 자료 구조를 구현하기 위한 참고 도서는 위튼(Witten) 등이 쓴 <Managing Gigabytes: Compressing and Indexing Documents and Images>


### 9.1.6 별칭(alias)와 로봇 순환

같은 문서를 가리키는 다른 URL이 있을 경우, 이전에 방문했는 지 여부를 체크하는 게 쉽지 않을 수 있다.


### 9.1.7 URL 정규화하기

1. 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환한다.
3. # 태그들을 제거한다.

URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만 이는 모든 URL 별칭을 커버하진 못한다.


### 9.1.8 파일 시스템 링크 순환

악의적으로 순환하는 시스템을 만들어서 로봇을 순환에 빠지게 할 수 있다.
예를 들면 htt://www.foo.com/subdir/subdir/subdir/index.html 이런 식으로.


### 9.1.9 동적 가상 웹 공간

웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 동적 콘텐츠를 통한 크롤러 함정을 만들 수도 있다.
콘텐츠의 동적 성질을 이해하지 못하는 로봇이라면 무한히 다음 콘텐츠를 요청할 수도 있다.


### 9.1.10 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없다. 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 '손실'을 유발할 수 있다.

웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법

* URL 정규화
* 너비 우선 크롤링
* 스로틀링
* URL 크기 제한
* URL/사이트 블랙리스트
* 패턴 발견
* 콘텐츠 지문(fingerprint)
* 사람의 모니터링



## 9.2 로봇의 HTTP

### 9.2.1 요청 헤더 식별하기

* User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
* From: 로봇의 사용자 / 관리자의 이메일 주소를 제공한다.
* Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
* Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공한다.


### 9.2.2 가상 호스팅

로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼져있는 현실에서, 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.


### 9.2.3 조건부 요청

수십억 개의 웹 페이지를 다운 받게 될 수도 있는 인터넷 검색 엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 한다. 
시간이나 엔터티 태그를 비교해서 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.


### 9.2.4 응답 다루기

대다수의 로봇은 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이라 응답 다루기라고 부를 만한 일은 거의 하지 않는다. 
하지만 조건부 요청과 같은 특정 몇몇 기능을 사용하거나 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알아야 한다.

* 상태 코드
* 엔터티


### 9.2.5 User-Agent 타겟팅

웹 관리자들은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고, 로봇들로부터의 요청을 예상해야 한다.
사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다. 


